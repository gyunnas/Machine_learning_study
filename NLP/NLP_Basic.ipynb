{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b5b8d4",
   "metadata": {},
   "source": [
    "### NLP와 텍스트 분석의 차이\n",
    "**NLP**: 머신이 인간의 언어를 이해하고 해석하는 데 더 중점을 두고 기술을 발전  \n",
    "**텍스트 마이닝**: 비정형 텍스트에서 의미 있는 정보를 추출하는 것에 좀 더 중점을 두고 기술을 발전\n",
    " 1) 텍스트 분류  \n",
    " 2) 감성분석  \n",
    " 3) 텍스트 요약  \n",
    " 4) 텍스트 군집화와 유사도 측정  \n",
    "\n",
    "### 텍스트 분석 이해\n",
    "**피처 벡터화(피처 추출)**: 텍스트를 word 기반의 다수의 피처로 추출하고 이 피처에 단어 빈도수와 같은 숫자 값을 부여 ex) BOW, Word2Vec  \n",
    "→ 머신러닝 알고리즘은 일반적으로 숫자형 피처를 데이터로 입력받아 동작하기 때문에 변환 필수!\n",
    "\n",
    "**텍스트 분석 수행 프로세스**\n",
    " 1) 텍스트 사전 준비 작업: 텍스트를 피처로 만들기 전에 클렌징, 대/소문자 변경, 특수문자 삭제 등의 클렌징 작업, 단어 등의 토큰화 작업, 의미없는 단어 제거 작업, 어근 추출(stemming/Lemmatization) 등의 텍스트 정규화 작업을 수행  \n",
    " 2) 피처 벡터화/추출: 사전 준비 작업으로 가공된 텍스트에서 피처를 추출하고 벡터값을 할당. BOW와 Word2Vec이 있으며, BOW는 Count 기반과 TF-IDF 기반 벡터화가 있음  \n",
    " 3) ML모델 수립 및 학습/예측/평가\n",
    " \n",
    "- NLTK, Gensim, SpaCy 등의 패키지를 사용\n",
    "\n",
    "**텍스트 정규화**: 텍스트를 머신러닝 알고리즘이나 NLP 애플리케이션에 입력 데이터로 사용하기 위해 클렌징, 정제, 토큰화, 어근화 등의 다양한 텍스트 데이터의 사전 작업을 수행하는 것을 의미  \n",
    "- 클렌징: 텍스트에서 분석에 방해가 되는 불필요한 문자, 기호 등을 사전에 제거\n",
    "    - stop word: 분석에 큰 의미가 없는 단어를 지칭 ex) is, the, a, will\n",
    "- 텍스트 토큰화: 문장토큰화, 단어토큰화가 있음\n",
    "- n_gram: 연속된 n개의 단어를 하나의 토큰화단위로 분리해 내는 것\n",
    "\n",
    "**Steming과 Lemmatization**: 문법적 또는 의미적으로 변화하는 단어의 원형을 찾는 것  \n",
    "  →Lemmatization이 품사와 같은 문법적인 요소와 더 의미적인 부분을 감안하여 정확한 철자로 된 어근 단어를 찾아주므로 Lemmatization이 Stemming보다 변환에 더 오랜 시간을 필요로 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6aaf59e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 3\n",
      "['The Matrix is everywhere its all around us, here even in this room.', 'You can see it out your window or on your television.', 'You feel it when you go to work, or go to church or pay your taxes.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\yoona\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# 문장 토큰화\n",
    "from nltk import sent_tokenize\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "text_sample = \"The Matrix is everywhere its all around us, here even in this room. \\\n",
    "                You can see it out your window or on your television. \\\n",
    "                You feel it when you go to work, or go to church or pay your taxes. \"\n",
    "\n",
    "sentences = sent_tokenize(text = text_sample)\n",
    "print(type(sentences), len(sentences))\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c405cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 15\n",
      "['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.']\n"
     ]
    }
   ],
   "source": [
    "# 단어 토큰화\n",
    "from nltk import word_tokenize\n",
    "\n",
    "sentence = \"The Matrix is everywhere its all around us, here even in this room.\"\n",
    "words = word_tokenize(sentence)\n",
    "print(type(words), len(words))\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22b80bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 3\n",
      "[['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.'], ['You', 'can', 'see', 'it', 'out', 'your', 'window', 'or', 'on', 'your', 'television', '.'], ['You', 'feel', 'it', 'when', 'you', 'go', 'to', 'work', ',', 'or', 'go', 'to', 'church', 'or', 'pay', 'your', 'taxes', '.']]\n"
     ]
    }
   ],
   "source": [
    "# 여러 개의 문장으로 된 입력 데이터를 문장별로 단어 토큰화하게 만드는 함수 생성\n",
    "def tokenize_text(text):\n",
    "    # 문장 별로 분리 토큰\n",
    "    sentences = sent_tokenize(text)\n",
    "    # 분리된 문장별 단어 토큰화\n",
    "    word_tokens = [word_tokenize(sentence) for sentence in sentences]\n",
    "    return word_tokens\n",
    "\n",
    "word_tokens = tokenize_text(text_sample)\n",
    "print(type(word_tokens), len(word_tokens))\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "139bc25a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 stop words 개수:  179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\yoona\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "print('영어 stop words 개수: ', len(nltk.corpus.stopwords.words('english')))\n",
    "print(nltk.corpus.stopwords.words('english')[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cba2ae88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['matrix', 'everywhere', 'around', 'us', ',', 'even', 'room', '.'], ['see', 'window', 'television', '.'], ['feel', 'go', 'work', ',', 'go', 'church', 'pay', 'taxes', '.']]\n"
     ]
    }
   ],
   "source": [
    "# word_tokens 리스트의 stopwords를 필터링으로 제거\n",
    "\n",
    "import nltk\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "all_tokens = []\n",
    "\n",
    "for sentence in word_tokens:\n",
    "    filtered_words = []\n",
    "    for word in sentence:\n",
    "        word = word.lower()\n",
    "        if word not in stopwords:\n",
    "            filtered_words.append(word)\n",
    "    all_tokens.append(filtered_words)\n",
    "\n",
    "print(all_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7627995e",
   "metadata": {},
   "source": [
    "### 3. Bag of Words\n",
    "\n",
    "**Bag of Words**: 문서가 가지는 모든 단어(words)를 문맥이나 순서를 무시하고 일괄적으로 단어에 대해 빈도값을 부여해 피처 값을 추출하는 모델. 문서 내 모든 단어를 한꺼번에 봉투 안에 넣은 뒤 흔들어서 섞는다는 의미로 Bag Of Words 모델이라고 함\n",
    "\n",
    "- 장점  \n",
    "    1) 쉽고 빠른 구축. 단순하지만 예상보다 문서의 특징을 잘 나타낼 수 있음\n",
    "\n",
    "- 단점  \n",
    "    1) 문맥 의미 반영 부족: 단어의 순서를 고려하지 않아 문장 내에서 단어의 문맥적인 의미 무시  \n",
    "    2) 희소 행렬 문제: 희소 행렬 형태의 데이터 세트가 만들어지므로 많은 문서에서 단어를 추출 시 매우 많은 단어가 칼럼으로 만들어짐\n",
    "\n",
    "cf) 희소 행렬: 대규모의 칼럼으로 구성된 행렬에서 대부분의 값이 0으로 채워지는 행렬  \n",
    "cf) 밀집 행렬: 대부분의 값이 0이 아닌 의미 있는 값으로 채워져 있는 행렬\n",
    "\n",
    "**BoW의 피처벡터화**  \n",
    "1) 카운트 기반의 벡터화: 해당 단어가 나타나는 횟수를 부여함  \n",
    "2) TF-IDF: 개별 문서에서 자주 나타나는 단어에 높은 가중치를 주되, 모든 문서에서 전반적으로 자주 나타나는 단어에 대해서는 페널티를 주는 방식으로 값을 부여  \n",
    " \n",
    "**사이킷런**: CountVectorizer, TfidfVectorizer로 Count 및 TF-IDF 벡터화 구현 가능   \n",
    " →사이킷런의 CountVectorizer나 TfidfVectorizer 클래스로 변환된 피처벡터화 행렬은 모두 사이파이의 CSR 형태의 희소 행렬임\n",
    "**희소행렬**\n",
    "1) COO 형식: 0이 아닌 데이터만 별도의 데이터배열에 저장하고, 그 데이터가 가리키는 행과 열의 위치를 별도의 배열로 저장하는 방식  \n",
    "2) CSR 형식: COO 형식이 행과 열의 위치를 나타내기 위해서 반복적인 위치 데이터를 사용해야 하는 문제점을 해결한 방식  \n",
    "    → COO 방식보다 메모리가 적게 들고 빠른 연산이 가능함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7c5c30a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 0, 1],\n",
       "       [0, 2, 0]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "\n",
    "dense = np.array([[3,0,1], [0,2,0]])\n",
    "\n",
    "# COO 방식\n",
    "data = np.array([3,1,2])\n",
    "row_pos = np.array([0,0,1])\n",
    "col_pos = np.array([0,2,1])\n",
    "\n",
    "sparse_coo = sparse.coo_matrix((data, (row_pos, col_pos)))\n",
    "sparse_coo.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6dcfdc23",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COO 변환된 데이터가 제대로 되었는지 다시 Dense로 출력 확인\n",
      "[[0 0 1 0 0 5]\n",
      " [1 4 0 3 2 5]\n",
      " [0 6 0 3 0 0]\n",
      " [2 0 0 0 0 0]\n",
      " [0 0 0 7 0 8]\n",
      " [1 0 0 0 0 0]]\n",
      "CSR 변환된 데이터가 제대로 되었는지 다시 Dense로 출력 확인\n",
      "[[0 0 1 0 0 5]\n",
      " [1 4 0 3 2 5]\n",
      " [0 6 0 3 0 0]\n",
      " [2 0 0 0 0 0]\n",
      " [0 0 0 7 0 8]\n",
      " [1 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "dense2 = np.array([[0, 0, 1, 0, 0, 5],\n",
    "                  [1, 4, 0, 3, 2, 5],\n",
    "                  [0, 6, 0, 3, 0, 0],\n",
    "                  [2, 0, 0, 0, 0, 0],\n",
    "                  [0, 0, 0, 7, 0, 8],\n",
    "                  [1, 0, 0, 0, 0, 0]])\n",
    "# CSR 방식\n",
    "data2 = np.array([1, 5, 1, 4, 3, 2, 5, 6, 3, 2, 7, 8, 1])\n",
    "\n",
    "# 행 위치와 열 위치를 각각 array로 생성\n",
    "row_pos = np.array([0, 0, 1, 1, 1, 1, 1, 2, 2, 3, 4, 4, 5])\n",
    "col_pos = np.array([2, 5, 0, 1, 3, 4, 5, 1, 3, 0, 3, 5, 0])\n",
    "\n",
    "# COO 형식으로 변환\n",
    "sparse_coo = sparse.coo_matrix((data2, (row_pos, col_pos)))\n",
    "\n",
    "# 행 위치 배열의 고유한 값의 시작 위치 인덱스를 배열로 생성\n",
    "row_pos_ind = np.array([0, 2, 7, 9, 10, 12, 13])\n",
    "\n",
    "# CSR 형식으로 변환\n",
    "sparse_csr = sparse.csr_matrix((data2, col_pos, row_pos_ind))\n",
    "\n",
    "print('COO 변환된 데이터가 제대로 되었는지 다시 Dense로 출력 확인')\n",
    "print(sparse_coo.toarray())\n",
    "print('CSR 변환된 데이터가 제대로 되었는지 다시 Dense로 출력 확인')\n",
    "print(sparse_csr.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69c155bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실제 사용 시에는 밀집행렬을 생성파라미터로 입력하면 COO나 CSR 희소 행렬로 생성\n",
    "dense3 = np.array([[0, 0, 1, 0, 0, 5],\n",
    "                  [1, 4, 0, 3, 2, 5],\n",
    "                  [0, 6, 0, 3, 0, 0],\n",
    "                  [2, 0, 0, 0, 0, 0],\n",
    "                  [0, 0, 0, 7, 0, 8],\n",
    "                  [1, 0, 0, 0, 0, 0]])\n",
    "coo = sparse.coo_matrix(dense3)\n",
    "csr = sparse.csr_matrix(dense3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
